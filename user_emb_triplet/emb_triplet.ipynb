{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc09026-703c-41db-b8aa-4e4f886af29b",
   "metadata": {},
   "source": [
    "# Что здесь происходит"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253ef2a-fbfd-469a-80a6-f2bccac0a55a",
   "metadata": {},
   "source": [
    "TL;DR построение эмбеддингов узлов графа, основываясь исключительно на информации о ребрах путем сближения векторов, между которыми есть ребра и отдаления векторов, между которыми ребра нет"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581bb316-c228-4974-aaab-29c236abde7b",
   "metadata": {},
   "source": [
    "## идея реализации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e4e15-49d2-440f-b50c-2afced4a2d98",
   "metadata": {},
   "source": [
    "Рассмотрим ненаправленный простой граф (без петель и кратных ребер) с произвольным количеством компонент связности, ребер и вершин. Требуется построить векторные представления вершин, основываясь на представлении о смежности этих вершин с другими. Для этого предлагается следующий несложный алгоритм:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd58bd-228f-4d69-8460-f0fff1d0774e",
   "metadata": {},
   "source": [
    "- составим матрицу смежности как пару (u_i, {v_j}), где {v_j} есть список вершин, смежных с u_i\n",
    "- инициализируем для каждого u_i случайный вектор e_i размера EMB_SIZE\n",
    "- элементами e_i будут целые числа из диапазона [-EMB_CARD/2; EMB_CARD/2]\n",
    "- для каждого u_i выберем* множество \"негативных\" вершин {n_j} |{n_j}| = N_i, где N_i = |{v_j}| таких, что расстояние D|u_i, n_j| минимально и n_j не принадлежит {v_j}\n",
    "- для каждой** тройки (u_i, v_j, n_j) посчитаем triplet_loss как max(0, D|u_i, v_j| - D|u_i, p_j| + MARGIN) и сдвинем u_i на расстояние ADAM_STEP в сторону получившегося градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192d3ab-a8b8-493e-b78c-2f68f559927a",
   "metadata": {},
   "source": [
    "\\* Для того, чтобы не перебирать все возможные вектора в поиске негативов для каждой из вершин за O(N*N), предлагается использовать LSH по евклидову расстоянию с репартиционированием. Так мы получим сложность O(N/PARTITIONS), поскольку для каждой партиции семплирование выполняется параллельно, а внутри одной партиции необходимо лишь перебрать все находящиеся на ней элементы 2 раза. Также "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd7ea2-d68c-4761-84eb-ff581364d9f7",
   "metadata": {},
   "source": [
    "** На самом деле не всегда для каждой, т.к. в общем случае степень вершины может быть достаточно большой. В этом случае мы берем не больше, чем LIMIT_POSITIVES положительных и LIMIT_NEGATIVES отрицательных примеров. Также для улучшения сходимости алгоритма было решено уменьшить число эпох EPOCHS в пользу увеличения циклов ITERATIONS - перестроение-repartition. У запускающего есть свобода выбора всех перечисленных гиперпараметров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ccfd1a-5663-47f0-a33d-0eee173719a2",
   "metadata": {},
   "source": [
    "Предложенный алгоритм был опробован на открытом датасете от FB https://snap.stanford.edu/data/ego-Facebook.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba50be-417c-412c-9a81-aa42efdbd6b7",
   "metadata": {},
   "source": [
    "в качестве меры качества полученных эмбеддингов мы берем recall@k, где k - это степень вершины, а 1/0 соответствует принадлежности к списку смежных вершин, все полученные результаты в графе *Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aeeb6d-dd3e-4081-9a64-9b75712341c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124dd6bc-52e8-43b9-bdde-3b5ad895d7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da48d61-28db-4988-82dc-6dcbed2c6c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "767de6ac-556d-4a99-b69c-23575b67fcca",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242c80d4-9a41-4a16-b580-19af2362e4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 16:23:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from datetime import date, datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"emb_via_triplet\")\\\n",
    ".config(\"spark.executor.memory\", \"4g\")\\\n",
    ".config(\"spark.driver.memory\", \"10g\")\\\n",
    ".config(\"spark.cores.max\", \"5\")\\\n",
    ".config(\"spark.yarn.am.cores\", \"5\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8183f3ad-2aad-4508-b273-8559ffe3e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5a57fb-e429-4750-b529-492caaddb0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 50\n",
    "EMB_CARD = 256\n",
    "PARTITIONS = 10\n",
    "\n",
    "EPOCHS = 2\n",
    "MARGIN = 5\n",
    "ADAM_STEP = 2.0\n",
    "LIMIT_POSITIVES = 400\n",
    "LIMIT_NEGATIVES = 300\n",
    "\n",
    "ITERATIONS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "327abaea-1a64-49c7-be6f-bbb8854e21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_cast(x):\n",
    "    arr = x._c0.split()\n",
    "    return (int(arr[0]), int(arr[1]))\n",
    "\n",
    "def calc_euclidean(x1, x2):\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "    return float(np.linalg.norm(x1 - x2))\n",
    "    \n",
    "def read_fb_dataset():\n",
    "    raw_csv = spark.read.csv(\"/facebook_combined.txt\")\n",
    "    return raw_csv.rdd.map(split_and_cast).toDF([\"user_id\", \"friend_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d002ae96-e5a0-4d86-a7b4-97c1678f8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_users(df):\n",
    "    return df.select(\"user_id\").distinct().unionAll(df.select(\"friend_id\").withColumnRenamed(\"user_id\", \"friend_id\").distinct()).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "730caf73-0654-4565-aee9-8b855d5029de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emb(x):\n",
    "    return (x.user_id, [random.randrange(int(-EMB_CARD/2), int(EMB_CARD/2)) for _ in range(EMB_SIZE)])\n",
    "\n",
    "def gen_random_embeddings(all_users):\n",
    "    return all_users.rdd.map(generate_emb).toDF([\"oid\", \"emb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670cbc5c-cc90-4cae-89a1-0438d4d93b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_emb_and_group_positives(df, emb_table):\n",
    "    df2 = df.join(emb_table, df.friend_id==emb_table.oid, \"inner\")\n",
    "    df3 = df2.drop(\"oid\").groupBy(\"user_id\").agg(F.collect_list(F.struct([\"friend_id\", \"emb\"])).alias(\"positives\"))\n",
    "    return df3.join(emb_table, df3.user_id==emb_table.oid, \"inner\").withColumnRenamed(\"emb\", \"owner_emb\").drop(\"oid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1d2cf3-b9c8-4f12-a578-fd636ed8f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repartition_by_dist(df):\n",
    "    df2 = df.withColumn(\"owner_emb_dense\", array_to_vector('owner_emb'))\n",
    "    brp = BucketedRandomProjectionLSH(inputCol=\"owner_emb_dense\", outputCol=\"hashes\", bucketLength=PARTITIONS, numHashTables=1)\n",
    "    model = brp.fit(df2)\n",
    "    df3 = model.transform(df2)\n",
    "    get_first=udf(lambda v: int(v[0]), IntegerType())\n",
    "    df4 = df3.withColumn(\"part\", get_first(F.col(\"hashes\").getItem(0))).drop(\"hashes\").drop(\"owner_emb_dense\")\n",
    "    return df4.repartition(\"part\").drop(\"part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "663a05d5-4fe4-451b-999a-720a4037491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_neg(all, uid, fr):\n",
    "    n = len(all)\n",
    "    neg = random.randrange(n)\n",
    "    if all[neg][0] in fr:\n",
    "        return None\n",
    "    return (all[neg][0], all[neg][1])\n",
    "\n",
    "def sample_negatives(iterator):\n",
    "    all_part = []\n",
    "    for it in iterator:\n",
    "        friends = set(map(lambda x: x[0], it.positives))\n",
    "        all_part.append((it.user_id, it.owner_emb, friends))\n",
    "    \n",
    "    res = []\n",
    "    n = len(all_part)\n",
    "    empty_limit = n\n",
    "    \n",
    "    for i in range(n):\n",
    "        uid = all_part[i][0]\n",
    "        fr = all_part[i][2]\n",
    "        negs = []\n",
    "        empty = 0\n",
    "        while len(negs) < len(fr) and empty < empty_limit:\n",
    "            next_neg = sample_next_neg(all_part, uid, fr)\n",
    "            if next_neg is None:\n",
    "                empty+=1\n",
    "            else:\n",
    "                negs.append(next_neg)\n",
    "        res.append((uid, negs))\n",
    "\n",
    "    return iter(res)\n",
    "\n",
    "def gen_negatives(df):\n",
    "    df2 = df.rdd.mapPartitions(sample_negatives).toDF([\"uid\", \"negatives\"])\n",
    "    return df.join(df2, df2.uid==df.user_id, \"inner\").drop(\"uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa5b3d88-f93e-4f2a-ada3-49bc41c9d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_arr_float(arr):\n",
    "    return list(map(lambda x: float(x), arr))\n",
    "\n",
    "def cast_arr_int(arr):\n",
    "    return list(map(lambda x: int(x), arr))\n",
    "\n",
    "def make_new_emb(x):\n",
    "    owner_emb = x.owner_emb\n",
    "    positives = random.sample(x.positives, min(LIMIT_POSITIVES, len(x.positives)))\n",
    "    negatives = random.sample(x.negatives, min(LIMIT_NEGATIVES, len(x.negatives)))\n",
    "    for _ in range(EPOCHS):\n",
    "        new_emb = make_new_emb_single_epoch(owner_emb, positives, negatives)\n",
    "        owner_emb = new_emb\n",
    "    return (x.user_id, owner_emb)\n",
    "    \n",
    "def make_new_emb_single_epoch(owner_emb, positives, negatives):\n",
    "    triplet_loss = nn.TripletMarginLoss(margin=MARGIN, p=2, eps=1e-7)\n",
    "    anchor = torch.tensor([cast_arr_float(owner_emb)], requires_grad=True)\n",
    "    for p in positives:\n",
    "        for n in negatives:\n",
    "            positive = torch.tensor([cast_arr_float(p.emb)], requires_grad=True)\n",
    "            negative = torch.tensor([cast_arr_float(n._2)], requires_grad=True)\n",
    "            embedding = nn.Embedding.from_pretrained(anchor, freeze=False)\n",
    "            e = embedding(torch.tensor([0]))\n",
    "            optimizer = torch.optim.Adam(embedding.parameters(), ADAM_STEP)\n",
    "            loss = triplet_loss(e, positive, negative)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return cast_arr_int(anchor.tolist()[0])\n",
    "\n",
    "def gen_new_emb(df):\n",
    "    return df.rdd.map(make_new_emb).toDF([\"oid\", \"emb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d15aa5-06ae-42ed-939c-4ec8a322db8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 16:23:33 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1 count:  3663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 187:===========>                                          (42 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "fb_ds = read_fb_dataset()\n",
    "all_users = extract_all_users(fb_ds)\n",
    "\n",
    "\n",
    "\n",
    "last_checkpoint = 0\n",
    "\n",
    "emb_table = gen_random_embeddings(all_users)\n",
    "emb_table.write.mode('overwrite').parquet(\"/emb_table_random\")\n",
    "# emb_table = spark.read.parquet(\"/emb_table_\"+str(last_checkpoint))\n",
    "\n",
    "for i in range(last_checkpoint+1, last_checkpoint+ITERATIONS+1):\n",
    "    df_w_pos = join_emb_and_group_positives(fb_ds, emb_table)\n",
    "    df_w_pos.persist()\n",
    "    df_w_pos.count()\n",
    "    \n",
    "    df_w_pos_repart = repartition_by_dist(df_w_pos)\n",
    "    df_w_pos_repart.persist()\n",
    "    df_w_pos_repart.count()\n",
    "    \n",
    "    df_final = gen_negatives(df_w_pos_repart)\n",
    "    df_final.persist()\n",
    "    df_final.count()\n",
    "\n",
    "    emb_table = gen_new_emb(df_final)\n",
    "    emb_table.persist()\n",
    "    print(\"iteration: \", i, \"count: \", emb_table.count())\n",
    "    \n",
    "    emb_table.write.mode('overwrite').parquet(\"/emb_table_\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7d7f4-b490-4163-9f5d-6505e2dd0f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e04bce7e-a1b6-4330-bc26-d707f9c4deeb",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36ac06-4845-45f6-b2af-ec2cf023b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_first_circle(len2_path):\n",
    "    return list(set(map(lambda x: x.first_circle_friend_id, len2_path)))\n",
    "\n",
    "def collect_second_circle(len2_path):\n",
    "    first_circle = collect_first_circle(len2_path)\n",
    "    second_circle = list(map(lambda x: x.second_circle_friend_id, len2_path)) + first_circle\n",
    "    return list(set(second_circle))\n",
    "\n",
    "def calc_euclidean(x1, x2):\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "    return float(np.linalg.norm(x1 - x2))\n",
    "\n",
    "def get_n_closest(owner_emb, n):\n",
    "    emb_table_clct_ = emb_table_clct_bc.value\n",
    "    map_to_dist = [ (x.oid, calc_euclidean(owner_emb, x.emb)) for x in emb_table_clct_]\n",
    "    map_to_dist_sorted = sorted(map_to_dist, key=lambda x: x[1])\n",
    "    return list(map(lambda x: x[0], map_to_dist_sorted))[0:n]\n",
    "\n",
    "def recall(n_closest, circle, n):\n",
    "    recall = 0\n",
    "    circle = set(circle)\n",
    "    for id in n_closest:\n",
    "        if id in circle:\n",
    "            recall+=1\n",
    "    return recall/n\n",
    "\n",
    "def calc_recalls_for_emb(emb_t):\n",
    "    fb_ds2 = read_fb_dataset().withColumnRenamed(\"friend_id\", \"first_circle_friend_id\").withColumnRenamed(\"user_id\", \"uid\")\n",
    "    fb_ds2 = read_fb_dataset()\n",
    "    fb_ds_cross = fb_ds.join(fb_ds2, fb_ds.first_circle_friend_id==fb_ds2.user_id, \"inner\")\\\n",
    "    .withColumnRenamed(\"friend_id\", \"second_circle_friend_id\").drop(\"user_id\")\\\n",
    "    .withColumnRenamed(\"uid\", \"user_id\")\n",
    "\n",
    "    fb_ds_cross.persist()\n",
    "    fb_ds_cross.count()\n",
    "\n",
    "    cols_list = [\"first_circle_friend_id\", \"second_circle_friend_id\"]\n",
    "    fb_ds_grp = fb_ds_cross.groupBy(\"user_id\").agg(F.collect_list(F.struct(cols_list)).alias(\"len2_path\"))\\\n",
    "    .join(emb_t, emb_t.oid==fb_ds_cross.user_id, \"inner\").drop(\"oid\")\\\n",
    "    .withColumnRenamed(\"emb\", \"owner_emb\")\n",
    "    \n",
    "    fb_ds_grp.persist()\n",
    "    fb_ds_grp.count()\n",
    "\n",
    "    collect_first_circle_udf = udf(collect_first_circle, ArrayType(IntegerType()))\n",
    "    collect_second_circle_udf = udf(collect_second_circle, ArrayType(IntegerType()))\n",
    "    \n",
    "    fb_ds_circles = fb_ds_grp.withColumn(\"first_circle\", collect_first_circle_udf(F.col(\"len2_path\")))\\\n",
    "    .withColumn(\"second_circle\", collect_second_circle_udf(F.col(\"len2_path\")))\\\n",
    "    .drop(\"len2_path\")\n",
    "    fb_ds_circles.persist()\n",
    "    fb_ds_circles.count()\n",
    "    \n",
    "    emb_table_clct = emb_t.collect()\n",
    "    emb_table_clct_bc = sc.broadcast(emb_table_clct)\n",
    "\n",
    "    get_n_closest_udf = udf(get_n_closest, ArrayType(IntegerType()))\n",
    "    fb_ds_closest = fb_ds_circles.withColumn(\"n_closest\", get_n_closest_udf(F.col(\"owner_emb\"), F.size(fb_ds_circles.first_circle)))\n",
    "    \n",
    "    fb_ds_closest.persist()\n",
    "    fb_ds_closest.count()\n",
    "\n",
    "    recall_udf = udf(recall, FloatType())\n",
    "    n_fc = F.size(fb_ds_circles.first_circle)\n",
    "    col_ncl = F.col(\"n_closest\")\n",
    "    \n",
    "    recall_df = fb_ds_closest.filter(\"size(first_circle) > 20\").withColumn(\"fc_recall\", recall_udf(col_ncl, F.col(\"first_circle\"), n_fc))\\\n",
    "    .withColumn(\"sc_recall\", recall_udf(col_ncl, F.col(\"second_circle\"), n_fc))\n",
    "    \n",
    "    recall_df.persist()\n",
    "    recall_df.count()\n",
    "\n",
    "    fc_recall = recall_df.agg({\"fc_recall\": \"avg\"}).head()[\"avg(fc_recall)\"]\n",
    "    sc_recall = recall_df.agg({\"sc_recall\": \"avg\"}).head()[\"avg(sc_recall)\"]\n",
    "\n",
    "    return (fc_recall, sc_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108cd8a-b0e8-468a-8c0e-51d5aa25327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_recalls_for_emb(spark.read.parquet(\"/emb_table_0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367df9b-716f-4483-9f86-c4371e5866d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_recalls_for_emb(spark.read.parquet(\"/emb_table_11\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2e092-14d2-49ec-9501-69993d61bd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b527085-ba7e-4f2a-9015-70325dd0dffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8f0fe-feec-46ad-b41a-9a762c149292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "941b368e-965c-453a-a9b1-a01e0899af05",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafad6c9-b933-4a81-aac4-346083340ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_uid = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82def6cc-ecb4-4062-a95c-3148b0f6e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "fb_ds = read_fb_dataset()\n",
    "# fb_ds.groupBy(\"user_id\").agg(F.count(\"friend_id\").alias(\"fr_cnt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8273d26-94b1-4f51-977e-3356c19074a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_ds = read_fb_dataset()\n",
    "sample = fb_ds.filter(\"user_id=\"+str(sample_uid))\n",
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58b9ea-68bf-45f8-9d40-87838ba8fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_friends = list(map(lambda x: x.friend_id, sample.select(\"friend_id\").collect()))\n",
    "len(sample_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ba9fd-0195-40dc-b2a3-307c64700f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c923bb-56cc-4121-98b5-d0db2c0d969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_table2 = spark.read.parquet(\"/emb_table_10\").withColumnRenamed(\"user_id\", \"oid\")\n",
    "emb_table = spark.read.parquet(\"/emb_table\")\n",
    "\n",
    "all_from_sample = sample.select(\"user_id\")\\\n",
    ".unionAll(sample.select(\"friend_id\").withColumnRenamed(\"friend_id\", \"user_id\")).distinct()\n",
    "\n",
    "all_from_sample_pls_emb = all_from_sample.join(emb_table, emb_table.oid==all_from_sample.user_id, \"inner\")\\\n",
    ".withColumnRenamed(\"emb\", \"emb_rnd\")\\\n",
    ".join(emb_table2, emb_table2.oid==all_from_sample.user_id, \"inner\")\\\n",
    ".withColumnRenamed(\"emb\", \"emb_1step\").drop(\"user_id\")\n",
    "\n",
    "all_from_sample_pls_emb.persist()\n",
    "all_from_sample_pls_emb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78357b21-a21f-46e2-8075-ddf3e4363399",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_from_sample_pls_emb_clctd = all_from_sample_pls_emb.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2b719-5f0d-4cbe-8bff-4d1e0aec53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rnd = list(map(lambda x: x.emb_rnd, all_from_sample_pls_emb_clctd))\n",
    "X_1step = list(map(lambda x: x.emb_1step, all_from_sample_pls_emb_clctd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44969c2d-32ae-4b17-a01d-7cd14f7b9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_owner_emb_1step = emb_table2.filter(\"oid=\"+str(sample_uid)).head().emb\n",
    "sample_owner_emb_1step_bc = sc.broadcast(sample_owner_emb_1step)\n",
    "calc_euclidean_to_owner_udf=udf(calc_euclidean_to_owner, FloatType())\n",
    "\n",
    "def calc_euclidean_to_owner(x2):\n",
    "    return calc_euclidean(sample_owner_emb_1step_bc.value, x2)\n",
    "\n",
    "def get_n_closest_except(df, n, except_):\n",
    "    top = df.sort(df.dist_to_owner.asc()).limit(len(except_) + n).collect()\n",
    "    res = []\n",
    "    i = 0\n",
    "    while len(res) < n and i < len(top):\n",
    "        item = top[i]\n",
    "        i+=1\n",
    "        if item.oid in except_:\n",
    "            continue\n",
    "        else:\n",
    "            res.append((item.oid, item.emb))\n",
    "    return res\n",
    "\n",
    "\n",
    "all_emb_w_dist_to_owner = emb_table2.withColumn(\"dist_to_owner\", calc_euclidean_to_owner_udf(F.col(\"emb\")))\n",
    "sample_closest = get_n_closest_except(all_emb_w_dist_to_owner, len(sample_friends), [])\n",
    "sample_closest_emb = list(map(lambda x: x[1], sample_closest))\n",
    "sample_closest_ids = list(map(lambda x: x[0], sample_closest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5a068-1961-4f3a-a428-85692f382c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=10)\n",
    "X_rnd_2d = tsne.fit_transform(np.array(X_rnd))\n",
    "X_1step_2d = tsne.fit_transform(np.array(X_1step))\n",
    "X_closest = tsne.fit_transform(np.array(sample_closest_emb))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "color_rnd = ['red'] + ['grey' for _ in range(len(X_rnd_2d) - 1)]\n",
    "color_1st = ['cyan'] + ['green' for _ in range(len(X_1step_2d) - 1)]\n",
    "ax1.scatter(X_rnd_2d[:,0], X_rnd_2d[:,1], color=color_rnd, label='rnd')\n",
    "ax1.scatter(X_1step_2d[:,0], X_1step_2d[:,1], color=color_1st, label='1st')\n",
    "ax1.scatter(X_closest[:,0], X_closest[:,1], color='blue', label='closest')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336b7b0-38c6-464e-af83-df978dec4944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b01243-e0d9-4202-8951-1c86c7f87cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11f755-47d6-46b5-a592-3aa75c48116c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b47f2-8700-42c5-86a8-1c1bd9d0f8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2da47e19-b563-4d36-8fd7-de30411da121",
   "metadata": {},
   "source": [
    "# Strange case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861cb78e-f46a-497d-b335-397947a4c9a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strange_embs = all_emb_w_dist_to_owner\\\n",
    ".filter(all_emb_w_dist_to_owner.oid.isin(sample_closest_ids) | all_emb_w_dist_to_owner.oid.isin(sample_friends))\\\n",
    ".withColumn(\"pid\", F.spark_partition_id())\\\n",
    ".sort(all_emb_w_dist_to_owner.dist_to_owner.asc())\n",
    "\n",
    "strange_embs.show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69a9e7-62d0-46ee-bded-319ee631d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_pos = join_emb_and_group_positives(fb_ds, strange_embs).sort(all_emb_w_dist_to_owner.dist_to_owner.asc())\\\n",
    ".withColumn(\"pid2\", F.spark_partition_id())\n",
    "df_w_pos.show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a724b-a569-4451-93ef-35e0d3ad025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_pos_repart = repartition_by_dist(df_w_pos)\\\n",
    ".sort(all_emb_w_dist_to_owner.dist_to_owner.asc())\\\n",
    ".withColumn(\"pid3\", F.spark_partition_id())\\\n",
    ".withColumn(\"is_fr\", df_w_pos_repart.user_id.isin(sample_friends))\n",
    "df_w_pos_repart.show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a9c9a1-dae3-4462-b3a5-8a3dca3d1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = gen_negatives(df_w_pos_repart)\\\n",
    ".sort(all_emb_w_dist_to_owner.dist_to_owner.asc())\\\n",
    ".withColumn(\"pid4\", F.spark_partition_id())\n",
    "\n",
    "df_final.show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83efc60-a84a-424b-97b7-de571c3607d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a76758-decc-4b2e-86d6-cda5593c2755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d65ed1c-b67d-4f0c-bc0d-60500bb8e17a",
   "metadata": {},
   "source": [
    "# EMB decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3765a8-202c-4e72-b6b9-34304d72a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_ds = read_fb_dataset()\n",
    "all_users = extract_all_users(fb_ds)\n",
    "emb_table = gen_random_embeddings(all_users)\n",
    "emb_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fe5a9-1ccb-43c2-9bd2-09de167754ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_pos = join_emb_and_group_positives(fb_ds, emb_table)\n",
    "df_w_pos.persist()\n",
    "df_w_pos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae670548-fcb5-4df8-9bea-6a236f37ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_pos_repart = repartition_by_dist(df_w_pos)\n",
    "df_w_pos_repart.persist()\n",
    "df_w_pos_repart.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c452a1b-4218-4ebf-aafe-d5d9e06a4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = gen_negatives(df_w_pos_repart)\n",
    "df_final.persist()\n",
    "df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d66549-ba7f-40d2-bce7-e189817826dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_table = gen_new_emb(df_final)\n",
    "    \n",
    "emb_table.write.mode('overwrite').parquet(\"/emb_table_0\")\n",
    "\n",
    "emb_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065eec9-3f8e-40ad-85e6-81aa96b08674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
