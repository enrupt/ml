{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc09026-703c-41db-b8aa-4e4f886af29b",
   "metadata": {},
   "source": [
    "# Что здесь происходит"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253ef2a-fbfd-469a-80a6-f2bccac0a55a",
   "metadata": {},
   "source": [
    "TL;DR построение эмбеддингов узлов графа, основываясь исключительно на информации о ребрах путем сближения векторов, между которыми есть ребра и отдаления векторов, между которыми ребра нет"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581bb316-c228-4974-aaab-29c236abde7b",
   "metadata": {},
   "source": [
    "## идея реализации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e4e15-49d2-440f-b50c-2afced4a2d98",
   "metadata": {},
   "source": [
    "Рассмотрим ненаправленный простой граф (без петель и кратных ребер) с произвольным количеством компонент связности, ребер и вершин. Требуется построить векторные представления вершин, основываясь на представлении о смежности одних вершин с другими. Для этого предлагается следующий несложный алгоритм:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd58bd-228f-4d69-8460-f0fff1d0774e",
   "metadata": {},
   "source": [
    "- составим матрицу смежности как пару (u_i, {v_j}), где {v_j} есть список вершин, смежных с u_i\n",
    "- инициализируем для каждого u_i случайный вектор e_i размера EMB_SIZE\n",
    "- элементами e_i будут целые числа из диапазона [-EMB_CARD/2; EMB_CARD/2]\n",
    "- для каждого u_i выберем* множество \"негативных\" вершин {n_j} |{n_j}| = N_i, где N_i = |{v_j}| таких, что расстояние D|u_i, n_j| минимально и n_j не принадлежит {v_j}\n",
    "- для каждой** тройки (u_i, v_j, n_j) посчитаем triplet_loss как max(0, D|u_i, v_j| - D|u_i, p_j| + MARGIN) и сдвинем u_i на расстояние ADAM_STEP в сторону получившегося градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192d3ab-a8b8-493e-b78c-2f68f559927a",
   "metadata": {},
   "source": [
    "\\* Для того, чтобы не перебирать все возможные вектора в поиске негативов для каждой из вершин за O(N*N), предлагается использовать LSH по евклидову расстоянию с репартиционированием. Так мы получим сложность O(N/PARTITIONS), поскольку для каждой партиции семплирование выполняется параллельно, а внутри одной партиции необходимо лишь перебрать все находящиеся на ней элементы 2 раза. Также "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd7ea2-d68c-4761-84eb-ff581364d9f7",
   "metadata": {},
   "source": [
    "** На самом деле не всегда для каждой, т.к. в общем случае степень вершины может быть достаточно большой. В этом случае мы берем не больше, чем LIMIT_POSITIVES положительных и LIMIT_NEGATIVES отрицательных примеров. Также для улучшения сходимости алгоритма было решено уменьшить число эпох EPOCHS в пользу увеличения циклов ITERATIONS - перестроение-repartition. У запускающего есть свобода выбора всех перечисленных гиперпараметров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ccfd1a-5663-47f0-a33d-0eee173719a2",
   "metadata": {},
   "source": [
    "Предложенный алгоритм был опробован на открытом датасете от FB https://snap.stanford.edu/data/ego-Facebook.html. Любопытно, что не все вершины данного графа, перечисленные в качестве смежных, присутствуют в качестве основных, что, учитывая двунаправленность связи, является противоречием. Данный факт приводит к любопытным спецэффектам: вершина при построении матрицы смежности не попадает в пайплайн и вектор для нее становится пустым и, в свою очередь, \"пропадает\" из списков смежности других вершин. Для некоторых вершин такие листья составляют весь список смежности и в итоге вершина сама становится листом, приводя к пропаданию новых. Датасет достраивается до полного дополнительно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba50be-417c-412c-9a81-aa42efdbd6b7",
   "metadata": {},
   "source": [
    "В качестве меры качества полученных эмбеддингов мы берем recall@k, где k - это степень вершины, а 1/0 соответствует принадлежности к списку смежных вершин, все полученные результаты в графе *Evaluation*. Уже на 4ой итерации полнота списка по второму кругу близка к 1. Количество итераций в конечном счете зависит от задачи, на данном этапе автор считает полученный показатель приемлемым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aeeb6d-dd3e-4081-9a64-9b75712341c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124dd6bc-52e8-43b9-bdde-3b5ad895d7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da48d61-28db-4988-82dc-6dcbed2c6c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "767de6ac-556d-4a99-b69c-23575b67fcca",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242c80d4-9a41-4a16-b580-19af2362e4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 20:45:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from datetime import date, datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"emb_via_triplet\")\\\n",
    ".config(\"spark.executor.memory\", \"4g\")\\\n",
    ".config(\"spark.driver.memory\", \"10g\")\\\n",
    ".config(\"spark.cores.max\", \"5\")\\\n",
    ".config(\"spark.yarn.am.cores\", \"5\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8183f3ad-2aad-4508-b273-8559ffe3e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, LongType, FloatType, ArrayType, StructType, StructField\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5a57fb-e429-4750-b529-492caaddb0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 60\n",
    "EMB_CARD = 256\n",
    "PARTITIONS = 20\n",
    "\n",
    "EPOCHS = 2\n",
    "MARGIN = 10\n",
    "ADAM_STEP = 2.0\n",
    "LIMIT_POSITIVES = 300\n",
    "LIMIT_NEGATIVES = 200\n",
    "\n",
    "ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327abaea-1a64-49c7-be6f-bbb8854e21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_cast(x):\n",
    "    arr = x._c0.split()\n",
    "    return (int(arr[0]), int(arr[1]))\n",
    "\n",
    "def calc_euclidean(x1, x2):\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "    return float(np.linalg.norm(x1 - x2))\n",
    "    \n",
    "def read_fb_dataset():\n",
    "    raw_csv = spark.read.csv(\"/facebook_combined.txt\")\n",
    "    return raw_csv.rdd.map(split_and_cast).toDF([\"user_id\", \"friend_id\"])\n",
    "\n",
    "def read_and_fix_fb_dataset():\n",
    "    fb_ds1 = read_fb_dataset()\n",
    "    fb_ds2 = read_fb_dataset()\n",
    "    fb_ds_inv = fb_ds2.withColumnRenamed(\"friend_id\", \"f\").withColumnRenamed(\"user_id\", \"friend_id\").withColumnRenamed(\"f\", \"user_id\")\n",
    "    return fb_ds1.union(fb_ds_inv.select(\"user_id\", \"friend_id\")).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002ae96-e5a0-4d86-a7b4-97c1678f8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_users(df):\n",
    "    return df.select(\"user_id\").distinct().unionAll(df.select(\"friend_id\").withColumnRenamed(\"user_id\", \"friend_id\").distinct()).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730caf73-0654-4565-aee9-8b855d5029de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emb(x):\n",
    "    return (x.user_id, [random.randrange(int(-EMB_CARD/2), int(EMB_CARD/2)) for _ in range(EMB_SIZE)])\n",
    "\n",
    "def gen_random_embeddings(all_users):\n",
    "    return all_users.rdd.map(generate_emb).toDF([\"oid\", \"emb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670cbc5c-cc90-4cae-89a1-0438d4d93b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_emb_and_group_positives(df, emb_table):\n",
    "    df2 = df.join(emb_table, df.friend_id==emb_table.oid, \"inner\")\n",
    "    df3 = df2.drop(\"oid\").groupBy(\"user_id\").agg(F.collect_list(F.struct([\"friend_id\", \"emb\"])).alias(\"positives\"))\n",
    "    df4 = df3.join(emb_table, df3.user_id==emb_table.oid, \"inner\").withColumnRenamed(\"emb\", \"owner_emb\").drop(\"oid\")\n",
    "    \n",
    "    # df5 = emb_table.join(df, df.user_id==emb_table.oid, \"left_anti\").withColumnRenamed(\"emb\", \"owner_emb\")\\\n",
    "    # .withColumnRenamed(\"oid\", \"user_id\")\\\n",
    "    # .withColumn('positives', F.array()).select(\"user_id\", \"positives\", \"owner_emb\")\n",
    "    # return df4.unionAll(df5)\n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d2cf3-b9c8-4f12-a578-fd636ed8f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repartition_by_dist(df):\n",
    "    df2 = df.withColumn(\"owner_emb_dense\", array_to_vector('owner_emb'))\n",
    "    brp = BucketedRandomProjectionLSH(inputCol=\"owner_emb_dense\", outputCol=\"hashes\", bucketLength=PARTITIONS, numHashTables=1)\n",
    "    model = brp.fit(df2)\n",
    "    df3 = model.transform(df2)\n",
    "    get_first=udf(lambda v: int(v[0]), IntegerType())\n",
    "    df4 = df3.withColumn(\"part_lsh\", get_first(F.col(\"hashes\").getItem(0))).drop(\"hashes\").drop(\"owner_emb_dense\")\n",
    "    return df4.repartition(PARTITIONS, \"part_lsh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "663a05d5-4fe4-451b-999a-720a4037491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_euclidean(x1, x2):\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "    return float(np.linalg.norm(x1 - x2))\n",
    "\n",
    "def get_n_closest_except(owner_emb, haystack, except_, n):\n",
    "    map_to_dist = [ (x.oid, x.emb, calc_euclidean(owner_emb, x.emb)) for x in haystack]\n",
    "    \n",
    "    map_to_dist_sorted = sorted(map_to_dist, key=lambda x: x[2])\n",
    "    \n",
    "    res = []\n",
    "    for id, emb, _dist in map_to_dist_sorted:\n",
    "        if id in except_:\n",
    "            continue\n",
    "        res.append((id, emb))\n",
    "        \n",
    "        if len(res) == n:\n",
    "            break\n",
    "\n",
    "    \n",
    "    return res\n",
    "        \n",
    "\n",
    "def sample_negatives(iterator):\n",
    "    all_part = []\n",
    "    Triplet = namedtuple('Triplet', 'oid emb friends')\n",
    "    for it in iterator:\n",
    "        friends = set(map(lambda x: x[0], it.positives))\n",
    "        all_part.append(Triplet(it.user_id, it.owner_emb, friends))\n",
    "    \n",
    "    res = []\n",
    "    n = len(all_part)\n",
    "    for i in range(n):\n",
    "        except_ = all_part[i].friends\n",
    "        except_.add(all_part[i].oid)\n",
    "        negs = get_n_closest_except(all_part[i].emb, all_part, set(except_), len(except_) - 1)\n",
    "        res.append((all_part[i].oid, negs))\n",
    "\n",
    "    return res\n",
    "\n",
    "def gen_negatives(df):\n",
    "    df2 = df.rdd.mapPartitions(sample_negatives).toDF([\"uid\", \"negatives\"])\n",
    "    return df.join(df2, df2.uid==df.user_id, \"left\").drop(\"uid\")\n",
    "\n",
    "# df_w_pos_repart_1 = spark.read.parquet(\"/df_w_pos_repart_1\")\n",
    "# df_w_pos_repart_1_part5 = df_final_2.filter(\"partition_id=5\").collect()\n",
    "# sample_negatives(df_w_pos_repart_1_part5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b3d88-f93e-4f2a-ada3-49bc41c9d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_arr_float(arr):\n",
    "    return list(map(lambda x: float(x), arr))\n",
    "\n",
    "def cast_arr_int(arr):\n",
    "    return list(map(lambda x: int(x), arr))\n",
    "\n",
    "def make_new_emb(x):\n",
    "    owner_emb = x.owner_emb\n",
    "    positives = random.sample(x.positives, min(LIMIT_POSITIVES, len(x.positives)))\n",
    "    negatives = random.sample(x.negatives, min(LIMIT_NEGATIVES, len(x.negatives)))\n",
    "    for _ in range(EPOCHS):\n",
    "        new_emb = make_new_emb_single_epoch(owner_emb, positives, negatives)\n",
    "        owner_emb = new_emb\n",
    "    return (x.user_id, owner_emb)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def make_new_emb_single_epoch(owner_emb, positives, negatives):\n",
    "\n",
    "    triplet_loss = nn.TripletMarginLoss(margin=MARGIN, p=2, eps=1e-7)\n",
    "    anchor = torch.tensor([cast_arr_float(owner_emb)], requires_grad=True)\n",
    "    \n",
    "    for p in positives:\n",
    "        for n in negatives:\n",
    "            positive = torch.tensor([cast_arr_float(p.emb)], requires_grad=True)\n",
    "            negative = torch.tensor([cast_arr_float(n._2)], requires_grad=True)\n",
    "            embedding = nn.Embedding.from_pretrained(anchor, freeze=False)\n",
    "            e = embedding(torch.tensor([0]))\n",
    "            optimizer = torch.optim.Adam(embedding.parameters(), ADAM_STEP)\n",
    "            loss = triplet_loss(e, positive, negative)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return cast_arr_int(anchor.tolist()[0])\n",
    "\n",
    "def gen_new_emb(df):\n",
    "    return df.rdd.map(make_new_emb).toDF([\"oid\", \"emb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9f6f4-1c8b-470b-8f9b-3237904bfb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d15aa5-06ae-42ed-939c-4ec8a322db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_ds = read_and_fix_fb_dataset()\n",
    "all_users = extract_all_users(fb_ds)\n",
    "\n",
    "last_checkpoint = 0\n",
    "\n",
    "emb_table = gen_random_embeddings(all_users)\n",
    "emb_table.write.mode('overwrite').parquet(\"/emb_table_random\")\n",
    "\n",
    "#emb_table = spark.read.parquet(\"/emb_table_\"+str(last_checkpoint))\n",
    "\n",
    "for i in range(last_checkpoint+1, last_checkpoint+ITERATIONS+1):\n",
    "    df_w_pos = join_emb_and_group_positives(fb_ds, emb_table)\n",
    "    df_w_pos.persist()\n",
    "    df_w_pos.count()\n",
    "    \n",
    "    df_w_pos_repart = repartition_by_dist(df_w_pos)\n",
    "    df_w_pos_repart.persist()\n",
    "    df_w_pos_repart.count()\n",
    "\n",
    "    df_w_pos_repart.write.mode('overwrite').parquet(\"/df_w_pos_repart_\"+str(i))\n",
    "    \n",
    "    df_final = gen_negatives(df_w_pos_repart)\n",
    "    df_final.persist()\n",
    "    df_final.count()\n",
    "\n",
    "    \n",
    "    df_final.withColumn(\"partition_id\", F.spark_partition_id()).write.mode('overwrite').parquet(\"/df_final_\"+str(i))\n",
    "    \n",
    "    emb_table = gen_new_emb(df_final)\n",
    "    emb_table.persist()\n",
    "    print(\"iteration: \", i, \"count: \", emb_table.count())\n",
    "    \n",
    "    emb_table.write.mode('overwrite').parquet(\"/emb_table_\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7d7f4-b490-4163-9f5d-6505e2dd0f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d159775-6185-4d9b-b3f4-3c1754cb7b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0052d3-75bc-40b0-94b1-03e287751c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e04bce7e-a1b6-4330-bc26-d707f9c4deeb",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a36ac06-4845-45f6-b2af-ec2cf023b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_first_circle(len2_path):\n",
    "    return list(set(map(lambda x: x.first_circle_friend_id, len2_path)))\n",
    "\n",
    "def collect_second_circle(len2_path):\n",
    "    first_circle = collect_first_circle(len2_path)\n",
    "    second_circle = list(map(lambda x: x.second_circle_friend_id, len2_path)) + first_circle\n",
    "    return list(set(second_circle))\n",
    "\n",
    "def recall(n_closest, circle, n):\n",
    "    recall = 0\n",
    "    circle = set(circle)\n",
    "    for id in n_closest:\n",
    "        if id in circle:\n",
    "            recall+=1\n",
    "    return recall/n\n",
    "\n",
    "def calc_recalls_for_emb(emb_t):\n",
    "    fb_ds = read_and_fix_fb_dataset().withColumnRenamed(\"friend_id\", \"first_circle_friend_id\").withColumnRenamed(\"user_id\", \"uid\")\n",
    "    fb_ds2 = read_and_fix_fb_dataset()\n",
    "\n",
    "    fb_ds_cross = fb_ds.join(fb_ds2, fb_ds.first_circle_friend_id==fb_ds2.user_id, \"inner\")\\\n",
    "    .withColumnRenamed(\"friend_id\", \"second_circle_friend_id\").drop(\"user_id\")\\\n",
    "    .withColumnRenamed(\"uid\", \"user_id\")\n",
    "\n",
    "    fb_ds_cross.persist()\n",
    "    fb_ds_cross.count()\n",
    "\n",
    "    cols_list = [\"first_circle_friend_id\", \"second_circle_friend_id\"]\n",
    "    fb_ds_grp = fb_ds_cross.groupBy(\"user_id\").agg(F.collect_list(F.struct(cols_list)).alias(\"len2_path\"))\\\n",
    "    .join(emb_t, emb_t.oid==fb_ds_cross.user_id, \"inner\").drop(\"oid\")\\\n",
    "    .withColumnRenamed(\"emb\", \"owner_emb\")\n",
    "    \n",
    "    fb_ds_grp.persist()\n",
    "    fb_ds_grp.count()\n",
    "\n",
    "    collect_first_circle_udf = udf(collect_first_circle, ArrayType(IntegerType()))\n",
    "    collect_second_circle_udf = udf(collect_second_circle, ArrayType(IntegerType()))\n",
    "\n",
    "    fb_ds_circles = fb_ds_grp.withColumn(\"first_circle\", collect_first_circle_udf(F.col(\"len2_path\")))\\\n",
    "    .withColumn(\"second_circle\", collect_second_circle_udf(F.col(\"len2_path\")))\\\n",
    "    .drop(\"len2_path\")\n",
    "    fb_ds_circles.persist()\n",
    "    fb_ds_circles.count()\n",
    "    \n",
    "    emb_table_clct = emb_t.collect()\n",
    "    emb_table_clct_bc = sc.broadcast(emb_table_clct)\n",
    "\n",
    "    def get_n_closest_to_owner(owner_id, owner_emb, n):\n",
    "        emb_table_clct_ = emb_table_clct_bc.value\n",
    "        id2emb_list = get_n_closest_except(owner_emb, emb_table_clct_, [owner_id], n)\n",
    "        return list(map(lambda x: x[0], id2emb_list))\n",
    "        \n",
    "    get_n_closest_to_owner_udf = udf(get_n_closest_to_owner, ArrayType(IntegerType()))\n",
    "    fb_ds_closest = fb_ds_circles.withColumn(\"n_closest\",\\\n",
    "                                             get_n_closest_to_owner_udf(F.col(\"user_id\"), \\\n",
    "                                                                        F.col(\"owner_emb\"), \\\n",
    "                                                                        F.size(fb_ds_circles.first_circle)))\n",
    "    \n",
    "    fb_ds_closest.persist()\n",
    "    fb_ds_closest.count()\n",
    "\n",
    "    recall_udf = udf(recall, FloatType())\n",
    "    n_fc = F.size(fb_ds_circles.first_circle)\n",
    "    col_ncl = F.col(\"n_closest\")\n",
    "    \n",
    "    recall_df = fb_ds_closest.filter(\"size(first_circle) > 20\")\\\n",
    "    .withColumn(\"fc_recall\", recall_udf(col_ncl, F.col(\"first_circle\"), n_fc))\\\n",
    "    .withColumn(\"sc_recall\", recall_udf(col_ncl, F.col(\"second_circle\"), n_fc))\n",
    "    \n",
    "    recall_df.persist()\n",
    "    recall_df.count()\n",
    "\n",
    "    recall_df.write.mode('overwrite').parquet(\"/fb_ds_grp_\")\n",
    "    \n",
    "    fc_recall = recall_df.agg({\"fc_recall\": \"avg\"}).head()[\"avg(fc_recall)\"]\n",
    "    sc_recall = recall_df.agg({\"sc_recall\": \"avg\"}).head()[\"avg(sc_recall)\"]\n",
    "\n",
    "    return (fc_recall, sc_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774f057-d00c-463a-bdf8-1a2c860e78fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4108cd8a-b0e8-468a-8c0e-51d5aa25327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.027570640367780393, 0.2205639165663426)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_recalls_for_emb(spark.read.parquet(\"/emb_table_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00ddd2fe-c242-437c-849f-4a7e8dcd9354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5392777759359398, 0.9997891434668658)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_recalls_for_emb(spark.read.parquet(\"/emb_table_4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f1f18-b9a7-40c1-aa71-9e1ca4982b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3413370-8484-425f-8cd8-b37a729b975b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8f0fe-feec-46ad-b41a-9a762c149292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "941b368e-965c-453a-a9b1-a01e0899af05",
   "metadata": {},
   "source": [
    "# TSNE (likely OOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafad6c9-b933-4a81-aac4-346083340ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_uid = 347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82def6cc-ecb4-4062-a95c-3148b0f6e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "fb_ds = read_fb_dataset()\n",
    "# fb_ds.groupBy(\"user_id\").agg(F.count(\"friend_id\").alias(\"fr_cnt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8273d26-94b1-4f51-977e-3356c19074a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_ds = read_and_fix_fb_dataset()\n",
    "sample = fb_ds.filter(\"user_id=\"+str(sample_uid)).cache()\n",
    "\n",
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58b9ea-68bf-45f8-9d40-87838ba8fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_friends = list(map(lambda x: x.friend_id, sample.select(\"friend_id\").collect()))\n",
    "len(sample_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ba9fd-0195-40dc-b2a3-307c64700f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c923bb-56cc-4121-98b5-d0db2c0d969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_table2 = spark.read.parquet(\"/emb_table_1\").withColumnRenamed(\"user_id\", \"oid\")\n",
    "emb_table = spark.read.parquet(\"/emb_table_random\")\n",
    "\n",
    "all_from_sample = sample.select(\"user_id\")\\\n",
    ".unionAll(sample.select(\"friend_id\").withColumnRenamed(\"friend_id\", \"user_id\")).distinct().cache()\n",
    "\n",
    "all_from_sample_pls_emb = all_from_sample.join(emb_table, emb_table.oid==all_from_sample.user_id, \"inner\")\\\n",
    ".withColumnRenamed(\"emb\", \"emb_rnd\")\\\n",
    ".join(emb_table2, emb_table2.oid==all_from_sample.user_id, \"inner\")\\\n",
    ".withColumnRenamed(\"emb\", \"emb_1step\").drop(\"user_id\").cache()\n",
    "\n",
    "all_from_sample_pls_emb.persist()\n",
    "all_from_sample_pls_emb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217dc947-059f-4586-83ba-bc565c0a352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_from_sample_pls_emb.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78357b21-a21f-46e2-8075-ddf3e4363399",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_from_sample_pls_emb_clctd = all_from_sample_pls_emb.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2b719-5f0d-4cbe-8bff-4d1e0aec53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rnd = list(map(lambda x: x.emb_rnd, all_from_sample_pls_emb_clctd))\n",
    "X_1step = list(map(lambda x: x.emb_1step, all_from_sample_pls_emb_clctd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44969c2d-32ae-4b17-a01d-7cd14f7b9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_owner_emb_1step = emb_table2.filter(\"oid=\"+str(sample_uid)).head().emb\n",
    "sample_owner_emb_1step_bc = sc.broadcast(sample_owner_emb_1step)\n",
    "\n",
    "def calc_euclidean_to_owner(x2):\n",
    "    return calc_euclidean(sample_owner_emb_1step_bc.value, x2)\n",
    "    \n",
    "calc_euclidean_to_owner_udf=udf(calc_euclidean_to_owner, FloatType())\n",
    "\n",
    "all_emb_w_dist_to_owner = emb_table2.withColumn(\"dist_to_owner\", calc_euclidean_to_owner_udf(F.col(\"emb\"))).cache()\n",
    "all_emb_w_dist_to_owner.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b14f5-4158-44e2-95e2-f21893d84273",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_closest = get_n_closest_except(sample_owner_emb_1step, all_emb_w_dist_to_owner.collect(), [], len(sample_friends))\n",
    "\n",
    "sample_closest_emb = list(map(lambda x: x[1], sample_closest))\n",
    "sample_closest_ids = list(map(lambda x: x[0], sample_closest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46eff2-7060-4fab-a2d7-42c73178d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5a068-1961-4f3a-a428-85692f382c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "X_rnd_2d = tsne.fit_transform(np.array(X_rnd))\n",
    "X_1step_2d = tsne.fit_transform(np.array(X_1step))\n",
    "X_closest = tsne.fit_transform(np.array(sample_closest_emb))\n",
    "\n",
    "# X_rnd_2d = np.array(X_rnd)\n",
    "# X_1step_2d = np.array(X_1step)\n",
    "# X_closest = np.array(sample_closest_emb)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "color_rnd = ['grey' for _ in range(len(X_rnd_2d))]\n",
    "color_1st = ['cyan'] + ['green' for _ in range(len(X_1step_2d) - 1)]\n",
    "ax1.scatter(X_rnd_2d[:,0], X_rnd_2d[:,1], color=color_rnd, label='rnd')\n",
    "ax1.scatter(X_1step_2d[:,0], X_1step_2d[:,1], color=color_1st, label='1st')\n",
    "#ax1.scatter(X_closest[:,0], X_closest[:,1], color='blue', label='closest')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336b7b0-38c6-464e-af83-df978dec4944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b01243-e0d9-4202-8951-1c86c7f87cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11f755-47d6-46b5-a592-3aa75c48116c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b47f2-8700-42c5-86a8-1c1bd9d0f8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
